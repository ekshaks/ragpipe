{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to build a simple RAG pipeline with **ragpipe** for the 'Founders Mode' essay by Paul Graham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape and parse text from the 'Founders Mode' essay html by Paul Graham, using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_doc_text():\n",
    "    url = 'https://paulgraham.com/foundermode.html'\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    for br in soup.find_all(\"br\"):\n",
    "            br.replace_with(\"\\n\")\n",
    "    text = soup.get_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the parsed document text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Founder Mode\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "September 2024\n",
      "\n",
      "At a YC event last week Brian Chesky gave a talk that everyone who\n",
      "was there will remember. Most founders I talked to afterward said\n",
      "it was the best they'd ever heard. Ron Conway, for the first time\n",
      "in his life, forgot to take notes. I'm not going to try to reproduce\n",
      "it here. Instead I want to talk about a question it raised.\n",
      "\n",
      "The theme of Brian's talk was that the conventional wisdom about\n",
      "how to run larger companies is mistaken. As Airbnb grew, well-meaning\n",
      "pe ....\n"
     ]
    }
   ],
   "source": [
    "dtext = get_doc_text()[:500]\n",
    "print(dtext, '....')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Data Model\n",
    "\n",
    "Now let's chunk the document text into paragraphs and store it in a (nested) dictionary *data model*. \n",
    "\n",
    "We will use a simple text splitter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from ragpipe.common import DotDict\n",
    "def build_data_model(text):\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    paragraphs = [dict(text=p.strip()) for p in paragraphs if p.strip()]\n",
    "\n",
    "    return DotDict(documents=paragraphs) #\n",
    "\n",
    "# We refer to D as the hierarchical document model. \n",
    "# It consists of a list of documents, where each document is a dictionary with a 'text' field.\n",
    "\n",
    "D = build_data_model(dtext) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a config yaml with representations and bridges.\n",
    "\n",
    "To find document chunks *relevant* to the user query, we will match the field `query.text` with `text` field of each document. The *config yaml* specifies how to match the query with doc's text fields.\n",
    "\n",
    "docpath notation:\n",
    "- we write hierarchical document fields in `jq` notation, call them *docpath*.\n",
    "- for example, in above data dictionary, the docpath for the `text` fields is `documents[].text`.\n",
    "\n",
    "In the config below, we specify how to match the query with doc's text fields.\n",
    "\n",
    "1. **Specify *representations* (reps) for both query and document fields.** using a dense encoder.\n",
    "    - Here, we define the representation (rep) named `dense` for both `query.text` and all chunk fields `documents[].text`. \n",
    "    - The reps are denoted as `query.text#dense` and `documents[].text#dense`.\n",
    "    - Both reps are created using the `BAAI/bge-small-en-v1.5` text encoder.\n",
    "\n",
    "2. **Specify one or more *bridge*s** over the reps.\n",
    "    - Here, we define bridge **b_dense** which matches `query.text#dense` and `documents[].text#dense` reps to find relevant doc paragraphs. \n",
    "    - The *output* of evaluating a bridge is a ranked list of documents ([(`documents.<number>.text`, `score`)]), containing up to `limit` documents. \n",
    "\n",
    "3. **Specify one or more *pipelines* (also called *merges*) over bridges**\n",
    "    - Enables building sequence-parallel combination of bridges. Not required for this simple example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_yaml = ''' \n",
    "encoders:\n",
    "  bge_small: #dense encoder\n",
    "    name: BAAI/bge-small-en-v1.5\n",
    "    query_instruction: \"Represent this sentence for searching relevant passages:\"\n",
    "  bm25: #not used in this example.\n",
    "    name: bm25\n",
    "\n",
    "representations:\n",
    "    query.text:\n",
    "        dense: {encoder: bge_small}\n",
    "\n",
    "    .documents[].text:\n",
    "        dense: {encoder: bge_small}\n",
    "\n",
    "bridges:\n",
    "  b_dense: #bridge over dense reps\n",
    "      repnodes: query.text#dense, .documents[].text#dense\n",
    "      limit: 10\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the config yaml and view the full config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bridges': {'b_dense': {'enabled': True,\n",
      "                         'limit': 10,\n",
      "                         'repnodes': ['query.text#dense',\n",
      "                                      '.documents[].text#dense']}},\n",
      " 'config_fname': '37230f31-f99f-5ae9-a53f-0e170b063e54',\n",
      " 'dbs': {'__default_multi_vector__': {'name': 'tensordb',\n",
      "                                      'options': {},\n",
      "                                      'path': '/tmp/ragpipe/'},\n",
      "         '__default_single_vector__': {'name': 'chromadb',\n",
      "                                       'options': {},\n",
      "                                       'path': '/tmp/ragpipe/'}},\n",
      " 'enabled_merges': ['_rp_m1_'],\n",
      " 'encoders': {'bge_small': {'dtype': '',\n",
      "                            'name': 'BAAI/bge-small-en-v1.5',\n",
      "                            'query_instruction': 'Represent this sentence for '\n",
      "                                                 'searching relevant passages:',\n",
      "                            'with_index': False},\n",
      "              'bm25': {'dtype': '', 'name': 'bm25', 'with_index': False}},\n",
      " 'etc': {},\n",
      " 'llm_models': {'__default__': 'groq/llama3-70b-8192'},\n",
      " 'merges': {'_rp_m1_': {'bridges': ['b_dense'],\n",
      "                        'expr': 'b_dense',\n",
      "                        'limit': 20,\n",
      "                        'method': 'expr'}},\n",
      " 'prompts': {},\n",
      " 'queries': [],\n",
      " 'representations': {'.documents[].text': {'dense': {'enabled': True,\n",
      "                                                     'encoder': {'dtype': '',\n",
      "                                                                 'name': 'BAAI/bge-small-en-v1.5',\n",
      "                                                                 'query_instruction': 'Represent '\n",
      "                                                                                      'this '\n",
      "                                                                                      'sentence '\n",
      "                                                                                      'for '\n",
      "                                                                                      'searching '\n",
      "                                                                                      'relevant '\n",
      "                                                                                      'passages:',\n",
      "                                                                 'with_index': False},\n",
      "                                                     'store': False}},\n",
      "                     'query.text': {'dense': {'enabled': True,\n",
      "                                              'encoder': {'dtype': '',\n",
      "                                                          'name': 'BAAI/bge-small-en-v1.5',\n",
      "                                                          'query_instruction': 'Represent '\n",
      "                                                                               'this '\n",
      "                                                                               'sentence '\n",
      "                                                                               'for '\n",
      "                                                                               'searching '\n",
      "                                                                               'relevant '\n",
      "                                                                               'passages:',\n",
      "                                                          'with_index': False},\n",
      "                                              'store': False}}}}\n"
     ]
    }
   ],
   "source": [
    "from ragpipe.config import load_config\n",
    "\n",
    "config = load_config(config_yaml, is_file=False, show=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve docs relevant to the query by evaluating the bridge `b_dense` on the data model `D`.\n",
    "View the retrieved docs in ranked order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eval Bridge(b_dense): repnodes=['query.text#dense', '.documents[].text#dense'] limit=10 enabled=True evalfn=None matchfn=None\n",
      "computing reps for query.text#dense...\n",
      "computing reps for .documents[].text#dense...\n",
      "Retrieving docs for Bridge b_dense...\n",
      "ENCODER =  name='BAAI/bge-small-en-v1.5' mo_loader=<function FastEmbedEncoder.from_config.<locals>.<lambda> at 0x104fd60c0> rep_type='single_vector' config=EncoderConfig(name='BAAI/bge-small-en-v1.5', prompt=None, query_instruction='Represent this sentence for searching relevant passages:', with_index=False, module=None, dtype='', size=None, shape=None)\n",
      "get_similarity_fn: BAAI/bge-small-en-v1.5\n",
      "Computing merge _rp_m1_...\n",
      "\n",
      "query: What is Founder mode?\n",
      "\n",
      "documents retrieved:\n",
      "\n",
      " ðŸ‘‰ 0.902  (documents.0.text) ðŸ‘‰  Founder Mode\n",
      " ðŸ‘‰ 0.731  (documents.7.text) ðŸ‘‰  There are as far as I know no books specifically about founder mode.\n",
      "Business schools don't know it exists. All we have so far are the\n",
      "experiments of individual founders who've been figuring it out for\n",
      "themselves. But now that we know what we're looking for, we can\n",
      "search for it. I hope in a few years founder mode will be as well\n",
      "understood as manager mode. We can already guess at some of the\n",
      "ways it will differ.\n",
      " ðŸ‘‰ 0.729  (documents.6.text) ðŸ‘‰  In effect there are two different ways to run a company: founder\n",
      "mode and manager mode. Till now most people even in Silicon Valley\n",
      "have implicitly assumed that scaling a startup meant switching to\n",
      "manager mode. But we can infer the existence of another mode from\n",
      "the dismay of founders who've tried it, and the success of their\n",
      "attempts to escape from it.\n",
      " ðŸ‘‰ 0.722  (documents.14.text) ðŸ‘‰  Indeed, another prediction I'll make about founder mode is that\n",
      "once we figure out what it is, we'll find that a number of individual\n",
      "founders were already most of the way there â€” except that in doing\n",
      "what they did they were regarded by many as eccentric or worse.\n",
      "[3]\n",
      " ðŸ‘‰ 0.719  (documents.15.text) ðŸ‘‰  Curiously enough it's an encouraging thought that we still know so\n",
      "little about founder mode. Look at what founders have achieved\n",
      "already, and yet they've achieved this against a headwind of bad\n",
      "advice. Imagine what they'll do once we can tell them how to run\n",
      "their companies like Steve Jobs instead of John Sculley.\n",
      " ðŸ‘‰ 0.701  (documents.19.text) ðŸ‘‰  [3]\n",
      "I also have another less optimistic prediction: as soon as\n",
      "the concept of founder mode becomes established, people will start\n",
      "misusing it. Founders who are unable to delegate even things they\n",
      "should will use founder mode as the excuse. Or managers who aren't\n",
      "founders will decide they should try to act like founders. That may\n",
      "even work, to some extent, but the results will be messy when it\n",
      "doesn't; the modular approach does at least limit the damage a bad\n",
      "CEO can do.\n",
      " ðŸ‘‰ 0.691  (documents.13.text) ðŸ‘‰  Obviously founders can't keep running a 2000 person company the way\n",
      "they ran it when it had 20. There's going to have to be some amount\n",
      "of delegation. Where the borders of autonomy end up, and how sharp\n",
      "they are, will probably vary from company to company. They'll even\n",
      "vary from time to time within the same company, as managers earn\n",
      "trust. So founder mode will be more complicated than manager mode.\n",
      "But it will also work better. We already know that from the examples\n",
      "of individual founders groping their way toward it.\n",
      " ðŸ‘‰ 0.685  (documents.12.text) ðŸ‘‰  For example, Steve Jobs used to run an annual retreat for what he\n",
      "considered the 100 most important people at Apple, and these were\n",
      "not the 100 people highest on the org chart. Can you imagine the\n",
      "force of will it would take to do this at the average company? And\n",
      "yet imagine how useful such a thing could be. It could make a big\n",
      "company feel like a startup. Steve presumably wouldn't have kept\n",
      "having these retreats if they didn't work. But I've never heard of\n",
      "another company doing this. So is it a good idea, or a bad one? We\n",
      "still don't know. That's how little we know about founder mode.\n",
      "[2]\n",
      " ðŸ‘‰ 0.664  (documents.11.text) ðŸ‘‰  Whatever founder mode consists of, it's pretty clear that it's going\n",
      "to break the principle that the CEO should engage with the company\n",
      "only via his or her direct reports. \"Skip-level\" meetings will\n",
      "become the norm instead of a practice so unusual that there's a\n",
      "name for it. And once you abandon that constraint there are a huge\n",
      "number of permutations to choose from.\n",
      " ðŸ‘‰ 0.610  (documents.9.text) ðŸ‘‰  Hire good people and give them room to do their jobs. Sounds great\n",
      "when it's described that way, doesn't it? Except in practice, judging\n",
      "from the report of founder after founder, what this often turns out\n",
      "to mean is: hire professional fakers and let them drive the company\n",
      "into the ground.\n"
     ]
    }
   ],
   "source": [
    "from ragpipe import Retriever\n",
    "\n",
    "query_text = 'What is Founder mode?'\n",
    "\n",
    "docs_retrieved = Retriever(config).eval(query_text, D)\n",
    "\n",
    "print(f'\\nquery: {query_text}')\n",
    "print(f'\\ndocuments retrieved:\\n')\n",
    "for doc in docs_retrieved: doc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the document snippets provided, \"Founder mode\" can be inferred to be a unique way of running a company, characterized by the continued involvement and leadership of the original founder(s), even as the company grows. It differs from \"manager mode\" which is typically associated with traditional business management practices. Founder mode emphasizes a more personal and direct engagement of the founder with various levels of the company, and may involve unconventional methods that may not fit into traditional business norms. However, the specifics of founder mode are not yet well-defined or understood, as it's a relatively new concept and there's little formal literature about it. It's also noted that as the concept becomes more established, there's a risk of it being misused or misunderstood.\n"
     ]
    }
   ],
   "source": [
    "prompt_templ = '''\n",
    "    The following are snippets from a document in markdown format.\n",
    "    # documents\n",
    "\n",
    "    {{documents}}\n",
    "\n",
    "    Answer the following query based on the above document snippets.\n",
    "\n",
    "    {{query}}\n",
    "    Answer:\n",
    "'''\n",
    "from ragpipe.llms import respond_to_contextual_query\n",
    "resp = respond_to_contextual_query(query_text, docs_retrieved, prompt_templ)\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional features\n",
    "\n",
    "That's it! This was a quick demo of how to build a naive RAG pipeline with ragpipe. For additional features, read below.\n",
    "\n",
    "We do a few more things to structure the code better:\n",
    "- specify one or more prompts under the `prompts` field in config.yml. Use them whenever interacting with an LLM.\n",
    "- similarly specify in config.yml:\n",
    "    - multiple queries with the `queries` field.\n",
    "    - multiple LLM providers under the `llm_models` field.\n",
    "- put together all the above functional units into a `Workflow` class - see `quickstart/project.py`\n",
    "\n",
    "Finally, if you are unhappy with the above answer, you can build upon this notebook in many ways to improve results:\n",
    "- add hybrid search (add sparse rep using bm25, add bridge b_sparse, merge using reciprocal rank fusion). See `examples/startups.yml`.\n",
    "- add storage, switch encoder to other models, add eval to compare.\n",
    "- for most practical usecases, we need at least two bridges over `..#dense` and `...#sparse` representations, whose outputs are merged to create the final ranked list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
