# This is a quickstart config example to get you started. 
# Edit the representations, bridges, merges, queries, etc. to customize.

prompts:
  qa1: |
    The following are snippets from a document in markdown format.
    # documents

    {{documents}}

    Answer the following query based on the above document snippets. 
    If the snippets cannot help answer the query, respond as <answer>Unknown</answer>

    {{query}}
    Answer:

dbs:
  chroma:
    path: /tmp/ragpipe/chroma/
    name: chromadb
  qdrant:
    path: /tmp/ragpipe/qdrant/
    name: qdrantdb
    options:
      vector: {distance: 'cosine'}
      optimizers: {memmap_threshold: 20000}
      hnsw: {on_disk: true}

encoders:
  bge_small:
    name: BAAI/bge-small-en-v1.5
    query_instruction: "Represent this sentence for searching relevant passages:"
  
  bm25:
    name: bm25
    with_index: true

  colbert:
    name: colbert-ir/colbertv2.0 
  mxlargebin:
    name: mxlargebin
    query_instruction: "Represent this sentence for searching relevant passages: "
    module: ext.libs.mxbai.MXLarge
    shape: {size: 512, dtype: 'ubinary'}

representations:
  dense: {encoder: bge_small}
  sparse: {encoder: bm25}

bridges:
  b:
    fields: query.text, .documents[].description
    reps: dense, sparse
    limit: 10


merges:
  c1:
    expr: b_dense
    limit: 10
  c2:
    method: reciprocal_rank
    bridges: b_dense, b_sparse
    limit: 10


enabled_merges: c2

llm_models:
  __default__: groq/llama3-70b-8192

etc:
  data_folder: ./data
  
queries:
  - query 1
  - query 2
  - query 3

