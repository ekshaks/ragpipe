{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ragpipe","text":"<p>Ragpipe helps you build tools to get insights from your large document repositories quickly by building fast RAG pipelines.</p> <p>Ragpipe makes it easy to tweak components of your RAG pipeline so that you can iterate fast until you get desired accurate responses.</p> <p>Instead of the usual <code>chunk-embed-match-rank</code> flow, Ragpipe adopts a holistic, end-to-end view of the pipeline, consisting of:</p> <ul> <li>building the data model, </li> <li>choosing representations for document parts, </li> <li>specifying the correct bridges among representations, </li> <li>merging the retrieved docs across bridges,</li> <li>and using the retrieved docs to compute the query response</li> </ul> <p>The <code>represent-bridge-merge</code> pattern is very powerful and allows us to build all kinds of complex retrieval engines with <code>retrieve-rank-rerank</code> patterns.</p>"},{"location":"#key-ideas","title":"Key Ideas","text":"<p>Representations. Choose the query/document fields as well as how to represent each chosen query / document field to aid similarity/relevance computation (bridges) over the entire document repository. Representations can be text strings, dense/sparse vector embeddings or arbitrary data objects, and help bridge the gap between the query and the documents.</p> <p>Bridges. Choose a pair of query and document representation to bridge. A bridge serves as a relevance indicator: one of the several criteria for identifying the relevant documents for a query. In practice, several bridges together determine the degree to which a document is relevant to a query. Computing each bridge creates a unique ranked list of documents.</p> <p>Merges. Specify how to combine the bridges, e.g., combine multiple ranked list of documents into a single ranked list.</p> <p>Data Model. A hierarchical data structure that consists of all the (nested) documents. The data model is created from the original document files and is retained over the entire pipeline. We compute representations for arbitrary nested fields of the data, without flattening the data tree.</p> <p>To query over a data repository, </p> <ul> <li>we compute the data model over the original data repository </li> <li>specify the document fields and the (multiple) representations to be computed for each field</li> <li>specify which representations to compute for query</li> <li>specify bridges: which pair of query and doc field representation should be matched</li> <li>merges: how to combine multiple bridges, sequentially or in parallel, to yield a curated ranked list of relevant documents.</li> <li>gen-response: how to generate response to the query using the relevant document list and a large language model.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>See the example in the <code>ragpipe/examples/insurance</code> directory. The <code>main</code> function from <code>insurance.py</code> is inlined below.</p> <pre><code>def main(respond_flag=False):\n    config = load_config('examples/insurance/insurance.yml', show=True) #L1\n\n    D = build_data_model('examples/data/insurance/niva-short.mmd') #L2\n\n    query_text = config.queries[1] #L3\n\n    from ragpipe.bridge import bridge_query_doc\n\n    docs_retrieved = bridge_query_doc(query_text, D, config) #L4\n\n    for doc in docs_retrieved: doc.show() #l5\n\n    if respond_flag:\n        return respond(query_text, docs_retrieved, config.prompts['qa2']) #L6\n    else:\n        return docs_retrieved\n\n</code></pre> <p>In <code>main</code>, we implement the following steps:</p> <ul> <li><code>#L1</code> read the config file <code>insurance.yml</code> which specifies representations, bridges and merges. <ul> <li>go through the <code>insurance.yml</code> file to understand the definitions.</li> </ul> </li> <li><code>#L2</code> read the data files (<code>niva-short.mmd</code>) to build a doc model <code>D</code> with following nested fields <ul> <li><code>query.text</code> which contains the query string </li> <li><code>.sections[].node</code> which contains the document snippets</li> </ul> </li> <li><code>#L4</code> all the heavy lifting happens in <code>ragpipe.bridge_query_doc</code> function based on definitions from <code>insurance.yml</code><ul> <li>compute <code>#dense</code> representations for <code>query.text</code> and <code>.sections[].node</code> fields using <code>colbert-ir/colbertv2.0</code></li> <li>rank documents according to the bridge <code>b1</code> across the two representations <code>query.text#dense</code>, <code>.sections[].node#dense</code></li> <li>compute a merge <code>c1</code> to obtain the final ranked list. In this case, the merge is trivial since only a single bridge is defined. </li> </ul> </li> <li><code>#L6</code> send the retrieved documents as context to the LLM, along with a QA prompt defined in <code>insurance.yml</code>; generate the final cohesive response.</li> </ul>"},{"location":"#faq","title":"FAQ","text":"<p>How / where do you chunk the long documents?</p> <p>Chunking happens during building the data model. Unlike conventional chunking, we do not create a flat \"list of text fields\" data model. Arbitrary parts of the document tree can be 'chunked' dynamically without losing the original document hierarchy. </p> <p>This makes it easy to explore different chunking strategies while retaining the unaffected parts of the downstream represent-bridge-merge pipeline.</p> <p>Which representations are supported?</p> <p>Ragpipe already includes a few popular dense and sparse vector encoders to get your pipeline started quickly. For example, BAAI/bge-small-en-v1.5, BM25, colbert-ir/colbertv2.0. The flexible configuration allows adding new external encoders by simply adding an <code>Encoder</code> class for the new encoder. Also, this allows us to keep the core of Ragpipe very lean.</p>"}]}